---
title: Train Qwen Demo
emoji: ğŸ¢
colorFrom: green
colorTo: purple
sdk: gradio
sdk_version: 6.3.0
app_file: app.py
pinned: false
license: mit
short_description: Training qwen light weight model
base_model: unsloth/Qwen2.5-0.5B-Instruct
tags:
- code
- reasoning
- fine-tuned
---

# Qwen2.5-0.5B-Instruct Fine-tuned with Reasoning Traces

## ğŸš€ Overview / æ¦‚è¿°
è¿™æ˜¯ä¸€ä¸ªåŸºäº Qwen2.5-0.5B-Instruct å¾®è°ƒçš„æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹æœ¬åœ°ä»£ç åº“çš„åˆ†æä¸è®¾è®¡åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚æ¨¡å‹åŒ…å«äº†è¯¦ç»†çš„**æ¨ç†è½¨è¿¹ (Reasoning Traces)**ï¼Œä½¿å…¶ä¸ä»…èƒ½å›ç­”â€œæ˜¯ä»€ä¹ˆâ€ï¼Œè¿˜èƒ½è§£é‡Šâ€œä¸ºä»€ä¹ˆâ€ã€‚

This is a model fine-tuned from Qwen2.5-0.5B-Instruct, optimized for local code analysis and architectural design. It includes detailed **Reasoning Traces**, enabling the model to not only answer "what" but also explain "why".

## ğŸ¯ Dataset / æ•°æ®é›†
æ¨¡å‹ä½¿ç”¨äº†ç”± `local-training-dataset-generator` è‡ªåŠ¨ç”Ÿæˆçš„ 1,400+ ä¸ªé«˜è´¨é‡æ ·æœ¬è¿›è¡Œå¾®è°ƒã€‚
The model was fine-tuned using 1,400+ high-quality samples generated by `local-training-dataset-generator`.

- **Scenario 1 (QA)**: Code logic analysis & explanation.
- **Scenario 2 (Design)**: Architectural design solutions based on requirements.

## ğŸš€ Training Steps (Remote) / è¿œç¨‹è®­ç»ƒæ­¥éª¤
ä½ å¯ä»¥ç›´æ¥åœ¨æ­¤ Space çš„ **"ğŸš€ Train"** æ ‡ç­¾é¡µè¿›è¡Œè¿œç¨‹å¾®è°ƒï¼š

1.  **ç¡¬ä»¶å‡†å¤‡**ï¼šç¡®ä¿ Space è¿è¡Œåœ¨ GPU ç¯å¢ƒï¼ˆæ¨è T4 æˆ–æ›´é«˜ï¼‰ã€‚
2.  **è®¾ç½® Token**ï¼šåœ¨ Space çš„ `Settings -> Variables and secrets` ä¸­æ·»åŠ  `HF_TOKEN` (éœ€å…·æœ‰ Write æƒé™)ã€‚
3.  **å¯åŠ¨è®­ç»ƒ**ï¼šåœ¨è®­ç»ƒç•Œé¢å¡«å…¥ç›®æ ‡ä»“åº“ IDï¼ˆé»˜è®¤å·²å¡«ï¼‰ï¼Œç‚¹å‡» **Start Training**ã€‚
4.  **ç»“æœäº§å‡º**ï¼š
    *   **LoRA æƒé‡**ï¼šè‡ªåŠ¨æ¨é€åˆ°ç›®æ ‡ä»“åº“çš„ `lora/` ç›®å½•ä¸‹ã€‚
    *   **GGUF æ¨¡å‹**ï¼šè‡ªåŠ¨å¯¼å‡ºå¹¶ä¸Šä¼ è‡³ `lora/` ç›®å½•ã€‚
    *   **è¯„æµ‹æŠ¥å‘Š**ï¼šè®­ç»ƒç»“æŸåä¼šè‡ªåŠ¨è¿è¡Œ 10 ä¸ªå…·æœ‰æ¢¯åº¦éš¾åº¦çš„æµ‹è¯•é¢˜ï¼Œç­”æ¡ˆä¿å­˜åœ¨ `lora/test_results.txt`ã€‚

> **âš ï¸ æ³¨æ„äº‹é¡¹ / Note**ï¼š
> - ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œå½“å‰ `train.py` ä¸­çš„ `max_steps` è®¾ç½®ä¸ºè¾ƒå°å€¼ï¼ˆå¦‚ 60 æ­¥ï¼‰ï¼Œåœ¨æ­£å¼ç”Ÿäº§ç¯å¢ƒä¸­åº”æ ¹æ®æ•°æ®é›†å¤§å°è¿›è¡Œè°ƒæ•´ã€‚
> - è®­ç»ƒæ•°æ®é›†è·¯å¾„é»˜è®¤è‡ªåŠ¨æœç´¢ `data/` ç›®å½•ä¸‹çš„ `.jsonl` æ–‡ä»¶ã€‚

## ğŸ’¬ Chat åŠŸèƒ½

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥åœ¨ **"ğŸ’¬ Chat"** æ ‡ç­¾é¡µä¸å¾®è°ƒåçš„æ¨¡å‹å¯¹è¯ã€‚æ¨¡å‹ä¼šè‡ªåŠ¨åŠ è½½ `lora/qwen2.5-0.5b-lora` æƒé‡ã€‚

> **âš ï¸ æ³¨æ„**: Chat å’Œ Test åŠŸèƒ½éœ€è¦ GPUã€‚è¯·ç¡®ä¿ Space è¿è¡Œåœ¨ GPU ç¯å¢ƒä¸‹ã€‚

## ğŸ§ª æµ‹è¯•æ¨ç†

æœ‰ä¸¤ç§æ–¹å¼è¿è¡Œæµ‹è¯•æ¨ç†ï¼š

### åœ¨ Hugging Face Space ä¸Šè¿è¡Œ

1. è¿›å…¥ **"ğŸ§ª Test"** æ ‡ç­¾é¡µ
2. **é…ç½®é€‰é¡¹**ï¼š
   - **Load Model From**ï¼ˆå¯é€‰ï¼‰ï¼š
     - ç•™ç©ºï¼šä½¿ç”¨æœ¬åœ° `lora/` ç›®å½•çš„æƒé‡ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åŸºç¡€æ¨¡å‹ï¼‰
     - å¡«å…¥ Hub Repo IDï¼ˆå¦‚ `xianyu564/train-qwen-demo`ï¼‰ï¼šç›´æ¥ä» Hub ä¸‹è½½å·²è®­ç»ƒçš„æƒé‡
   - **Upload Results To**ï¼ˆé»˜è®¤å·²å¡«ï¼‰ï¼š
     - æŒ‡å®šç»“æœä¸Šä¼ åˆ°å“ªä¸ª Hub ä»“åº“
     - é»˜è®¤å€¼ï¼š`xianyu564/train-qwen-demo`
3. ç‚¹å‡» **"Run Test Inference"** æŒ‰é’®
4. ç³»ç»Ÿä¼šï¼š
   - è‡ªåŠ¨åŠ è½½æŒ‡å®šçš„æ¨¡å‹
   - å¯¹ `data/test_questions_fullLength.jsonl` ä¸­çš„ 10 ä¸ªé—®é¢˜è¿›è¡Œæ¨ç†ï¼ˆçº¦ 7-8 åˆ†é’Ÿï¼‰
   - ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„ç»“æœæ–‡ä»¶ï¼ˆä¸è¦†ç›–å†å²ï¼‰
   - è‡ªåŠ¨ä¸Šä¼ åˆ°æŒ‡å®šçš„ Hub ä»“åº“
5. æŸ¥çœ‹ç»“æœï¼š
   - ç•Œé¢æ˜¾ç¤ºå‰ 2 ä¸ªé—®é¢˜çš„é¢„è§ˆ
   - Hub ä»“åº“çš„ `lora/` ç›®å½•ä¸‹ä¼šæœ‰å®Œæ•´ç»“æœ

> ğŸ’¡ **æç¤º**: 
> - å¦‚æœä½ ä¹‹å‰è®­ç»ƒè¿‡æ¨¡å‹å¹¶æ¨é€åˆ° Hubï¼Œå¯ä»¥ç›´æ¥è¾“å…¥ Hub Repo ID æ¥åŠ è½½æƒé‡ï¼Œ**æ— éœ€é‡æ–°è®­ç»ƒ**ï¼
> - æ¯æ¬¡æµ‹è¯•éƒ½ä¼šç”Ÿæˆæ–°çš„æ—¶é—´æˆ³æ–‡ä»¶ï¼Œå†å²è®°å½•ä¸ä¼šè¢«è¦†ç›–
> - ç¡®ä¿ Space çš„ `HF_TOKEN` Secret å·²è®¾ç½®ï¼Œæ‰èƒ½ä¸Šä¼ ç»“æœ

### æœ¬åœ°è¿è¡Œ

```bash
python run_inference.py
```

æµ‹è¯•æ•°æ®æ ¼å¼ï¼š
- **`data/test_questions.csv`**: åŸå§‹ç®€çŸ­ç‰ˆæµ‹è¯•é—®é¢˜ï¼ˆç”¨äºè®­ç»ƒåè‡ªåŠ¨æµ‹è¯•ï¼‰
- **`data/test_questions_fullLength.jsonl`**: å®Œæ•´ç‰ˆæµ‹è¯•é—®é¢˜ï¼ˆç›´æ¥æ¥è‡ªéªŒè¯é›†ï¼ŒåŒ…å«å®Œæ•´ä»£ç ï¼‰
- **`data/test_mapping.md`**: é—®é¢˜ä¸éªŒè¯é›†çš„å¯¹åº”å…³ç³»

## ğŸ“ è¾“å‡ºç»“æ„

è®­ç»ƒå®Œæˆåï¼Œæ‰€æœ‰è¾“å‡ºå°†ä¿å­˜åœ¨ `lora/` ç›®å½•ï¼š

- **`lora/qwen2.5-0.5b-lora/`**: LoRA é€‚é…å™¨æƒé‡
- **`lora/model_gguf/`**: GGUF æ ¼å¼æ¨¡å‹ï¼ˆq8_0 é‡åŒ–ï¼‰
- **`lora/test_results.txt`**: è®­ç»ƒåè‡ªåŠ¨æµ‹è¯•ç»“æœï¼ˆç®€çŸ­ç‰ˆé—®é¢˜ï¼‰
- **`lora/test_results_fullLength_YYYYMMDD_HHMMSS.txt`**: å®Œæ•´æµ‹è¯•ç»“æœï¼ˆå¸¦æ—¶é—´æˆ³ï¼Œä¸è¦†ç›–ï¼‰
- **`lora/test_results_fullLength_latest.txt`**: æœ€æ–°çš„å®Œæ•´æµ‹è¯•ç»“æœ
- **`lora/logs/`**: TensorBoard è®­ç»ƒæ—¥å¿—
- **`lora/outputs/`**: Trainer çŠ¶æ€å’Œæ£€æŸ¥ç‚¹

> ğŸ’¡ **æ–‡ä»¶å‘½åè¯´æ˜**ï¼š
> - æ¯æ¬¡è¿è¡Œå®Œæ•´æµ‹è¯•éƒ½ä¼šç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–°æ–‡ä»¶ï¼Œä¿ç•™å†å²è®°å½•
> - `_latest.txt` æ–‡ä»¶å§‹ç»ˆæŒ‡å‘æœ€æ–°çš„æµ‹è¯•ç»“æœï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥çœ‹

## ğŸ› ï¸ Usage / ä½¿ç”¨
ä½ å¯ä»¥ç›´æ¥é€šè¿‡ Hugging Face `transformers` åº“åŠ è½½æ­¤æ¨¡å‹ï¼ˆé…å¥— LoRA æƒé‡ï¼‰ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model = "unsloth/Qwen2.5-0.5B-Instruct"
lora_model = "./qwen2.5-0.5b-lora"

model = AutoModelForCausalLM.from_pretrained(base_model)
model = PeftModel.from_pretrained(model, lora_model)
```

## ğŸ“ˆ Training Info / è®­ç»ƒç»Ÿè®¡
- **Base Model**: Qwen2.5-0.5B-Instruct
- **Method**: LoRA (Rank 16)
- **Framework**: Unsloth
- **Avg Reasoning Steps**: 6.05

---
Generated by [Local Training Dataset Generator](https://github.com/xianyu564/local-training-dataset-generator)
