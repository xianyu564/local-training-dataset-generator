---
license: mit
base_model: unsloth/Qwen2.5-0.5B-Instruct
tags:
- code
- reasoning
- fine-tuned
---

# Qwen2.5-0.5B-Instruct Fine-tuned with Reasoning Traces

## ğŸš€ Overview / æ¦‚è¿°
è¿™æ˜¯ä¸€ä¸ªåŸºäº Qwen2.5-0.5B-Instruct å¾®è°ƒçš„æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹æœ¬åœ°ä»£ç åº“çš„åˆ†æä¸è®¾è®¡åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚æ¨¡å‹åŒ…å«äº†è¯¦ç»†çš„**æ¨ç†è½¨è¿¹ (Reasoning Traces)**ï¼Œä½¿å…¶ä¸ä»…èƒ½å›ç­”â€œæ˜¯ä»€ä¹ˆâ€ï¼Œè¿˜èƒ½è§£é‡Šâ€œä¸ºä»€ä¹ˆâ€ã€‚

This is a model fine-tuned from Qwen2.5-0.5B-Instruct, optimized for local code analysis and architectural design. It includes detailed **Reasoning Traces**, enabling the model to not only answer "what" but also explain "why".

## ğŸ¯ Dataset / æ•°æ®é›†
æ¨¡å‹ä½¿ç”¨äº†ç”± `local-training-dataset-generator` è‡ªåŠ¨ç”Ÿæˆçš„ 1,400+ ä¸ªé«˜è´¨é‡æ ·æœ¬è¿›è¡Œå¾®è°ƒã€‚
The model was fine-tuned using 1,400+ high-quality samples generated by `local-training-dataset-generator`.

- **Scenario 1 (QA)**: Code logic analysis & explanation.
- **Scenario 2 (Design)**: Architectural design solutions based on requirements.

## ğŸ› ï¸ Usage / ä½¿ç”¨
ä½ å¯ä»¥ç›´æ¥é€šè¿‡ Hugging Face `transformers` åº“åŠ è½½æ­¤æ¨¡å‹ï¼ˆé…å¥— LoRA æƒé‡ï¼‰ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model = "unsloth/Qwen2.5-0.5B-Instruct"
lora_model = "./qwen2.5-0.5b-lora"

model = AutoModelForCausalLM.from_pretrained(base_model)
model = PeftModel.from_pretrained(model, lora_model)
```

## ğŸ“ˆ Training Info / è®­ç»ƒç»Ÿè®¡
- **Base Model**: Qwen2.5-0.5B-Instruct
- **Method**: LoRA (Rank 16)
- **Framework**: Unsloth
- **Avg Reasoning Steps**: 6.05

---
Generated by [Local Training Dataset Generator](https://github.com/xianyu564/local-training-dataset-generator)
