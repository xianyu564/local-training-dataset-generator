"""
Pipeline Workflow - Automated End-to-End Training Dataset Generation
è‡ªåŠ¨åŒ–ç«¯åˆ°ç«¯è®­ç»ƒæ•°æ®é›†ç”Ÿæˆæµæ°´çº¿

This workflow automates the entire process from code slicing to dataset compilation,
skipping the manual review stage for direct processing.
æ­¤å·¥ä½œæµè‡ªåŠ¨åŒ–äº†ä»ä»£ç åˆ‡ç‰‡åˆ°æ•°æ®é›†ç¼–è¯‘çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œè·³è¿‡äººå·¥å®¡æ ¸é˜¶æ®µç›´æ¥è¿›è¡Œå¤„ç†ã€‚
"""

import sys
import logging
import json
import shutil
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# Add project root to path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.pipeline.code_slicer import CodeSlicer
from src.pipeline.scenario_processor import ScenarioProcessor
from src.pipeline.batch_submitter import BatchSubmitter
from src.pipeline.dataset_compiler import DatasetCompiler

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("PipelineWorkflow")

def main():
    # 1. Configuration / é…ç½®
    # ========================================================================
    REPOS_ROOT = Path("data/0.cloned_repo")
    SLICES_ROOT = Path("data/1.slices")
    BATCH_INPUT_ROOT = Path("data/3.batch_input")
    BATCH_OUTPUT_ROOT = Path("data/4.batch_output")
    FINAL_OUTPUT_ROOT = Path("data/5.final_output")
    
    CONFIG_PATH = "config.json"
    MAX_FILES_PER_REPO = 100 # Adjust as needed
    
    # Ensure directories exist
    for d in [SLICES_ROOT, BATCH_INPUT_ROOT, BATCH_OUTPUT_ROOT, FINAL_OUTPUT_ROOT]:
        d.mkdir(parents=True, exist_ok=True)

    print("=" * 70)
    print("ğŸš€ Elephenotype: Training Dataset Generation Pipeline")
    print("ğŸš€ Elephenotype: è®­ç»ƒæ•°æ®é›†ç”Ÿæˆæµæ°´çº¿ (è‡ªåŠ¨åŒ–ç‰ˆ)")
    print("=" * 70)

    # 2. STAGE 1: Code Slicing / ä»£ç åˆ‡ç‰‡
    # ========================================================================
    print(f"\n[STAGE 1] Code Slicing / ä»£ç åˆ‡ç‰‡")
    print("-" * 70)
    
    repo_dirs = [d for d in REPOS_ROOT.iterdir() if d.is_dir()]
    if not repo_dirs:
        logger.error(f"No repositories found in {REPOS_ROOT}. Please clone some repos first.")
        return

    slicer = CodeSlicer()
    all_slices_paths = []

    for repo_path in repo_dirs:
        repo_name = repo_path.name
        logger.info(f"Slicing repository: {repo_name}")
        
        # Slice repository
        repo_slices = slicer.slice_repository(
            repo_path=str(repo_path),
            repo_name=repo_name,
            max_files=MAX_FILES_PER_REPO
        )
        
        # Export to data/1.slices/{repo_name}/code_slices.jsonl
        repo_output_dir = SLICES_ROOT / repo_name
        repo_output_dir.mkdir(parents=True, exist_ok=True)
        slices_file = slicer.export_slices(output_file=repo_output_dir / "code_slices.jsonl")
        all_slices_paths.append(slices_file)
        
        # Clear slicer's internal state for next repo
        slicer.slices = []

    # 3. STAGE 2: Scenario Processing / åœºæ™¯å¤„ç† (è·³è¿‡äººå·¥å®¡æ ¸)
    # ========================================================================
    # ç›´æ¥ä½¿ç”¨ data/1.slices ä½œä¸ºè¾“å…¥ï¼Œè·³è¿‡ data/2.reviewed_slices
    print(f"\n[STAGE 2] Scenario Processing / åœºæ™¯å¤„ç† (Skipping Manual Review)")
    print("-" * 70)
    
    processor = ScenarioProcessor(config_path=CONFIG_PATH)
    
    for repo_name in [d.name for d in repo_dirs]:
        logger.info(f"Processing scenarios for: {repo_name}")
        
        # Use slices from Stage 1 directly
        repo_slices_dir = SLICES_ROOT / repo_name
        repo_batch_input_dir = BATCH_INPUT_ROOT / repo_name
        repo_batch_input_dir.mkdir(parents=True, exist_ok=True)
        
        # Update processor output dir for this repo
        processor.output_dir = repo_batch_input_dir
        
        # Process slices into batch inputs
        # Note: We pass SLICES_ROOT/{repo_name} as the "reviewed" directory
        batch_files = processor.process_reviewed_slices(
            reviewed_slices_dir=str(repo_slices_dir),
            max_scenario1=200, # Example limits
            max_scenario2=100
        )
        logger.info(f"Generated batch inputs for {repo_name}: {list(batch_files.keys())}")

    # 4. STAGE 3: Batch Submission / æ‰¹å¤„ç†æäº¤
    # ========================================================================
    print(f"\n[STAGE 3] Batch Submission / æ‰¹å¤„ç†æäº¤")
    print("-" * 70)
    
    # Check if config has API key
    try:
        submitter = BatchSubmitter(config_path=CONFIG_PATH)
        user_input = input("Do you want to submit these batches to OpenAI now? (y/n): ")
        
        if user_input.lower() == 'y':
            for repo_name in [d.name for d in repo_dirs]:
                repo_input_dir = BATCH_INPUT_ROOT / repo_name
                repo_output_dir = BATCH_OUTPUT_ROOT / repo_name
                
                logger.info(f"Submitting batches for {repo_name}...")
                submission_results = submitter.submit_batch_files(
                    batch_input_dir=str(repo_input_dir),
                    output_dir=str(repo_output_dir)
                )
                logger.info(f"Submitted {len(submission_results['submitted_jobs'])} jobs for {repo_name}")
        else:
            print("Skipping submission. You can submit later using:")
            print("python src/pipeline/batch_submitter.py")
    except Exception as e:
        logger.error(f"Batch submission setup failed (possibly missing config.json or API key): {e}")
        print("Skipping automatic submission.")

    # 5. STAGE 4: Dataset Compilation / æ•°æ®é›†ç¼–è¯‘
    # ========================================================================
    print(f"\n[STAGE 4] Dataset Compilation / æ•°æ®é›†ç¼–è¯‘")
    print("-" * 70)
    print("NOTE: This stage requires batch results in data/4.batch_output.")
    
    # We check if there are any results to compile
    output_files = list(BATCH_OUTPUT_ROOT.rglob("scenario*_output.jsonl"))
    
    if not output_files:
        print("No batch results found in data/4.batch_output.")
        print("Please download results after they are processed by OpenAI and place them in the directory structure:")
        print("data/4.batch_output/{repo_name}/scenarioX_output.jsonl")
    else:
        logger.info(f"Found {len(output_files)} result files. Starting compilation...")
        
        # DatasetCompiler uses source_data_dir for mapping
        # Since we skipped manual review, we point it to SLICES_ROOT (data/1.slices)
        compiler = DatasetCompiler(
            output_dir=str(FINAL_OUTPUT_ROOT),
            source_data_dir=str(SLICES_ROOT)
        )
        
        results = compiler.process_all_outputs(
            batch_output_dir=str(BATCH_OUTPUT_ROOT),
            train_ratio=0.8,
            val_ratio=0.2
        )
        
        print("\nâœ… Compilation Complete!")
        print(f"   Train Dataset: {results['unified_datasets']['train']}")
        print(f"   Val Dataset:   {results['unified_datasets']['val']}")
        print(f"   Statistics:    {results['statistics_file']}")
        print(f"   Summary:       {results['summary_file']}")

    print("\n" + "=" * 70)
    print("ğŸ Workflow execution finished / å·¥ä½œæµæ‰§è¡Œå®Œæ¯•")
    print("=" * 70)

if __name__ == "__main__":
    main()
