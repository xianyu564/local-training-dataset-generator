# æ¨¡å‹å¾®è°ƒ / Model Fine-tuning

æœ¬é¡¹ç›®ä¸ä»…æä¾›æ•°æ®ç”Ÿæˆèƒ½åŠ›ï¼Œè¿˜åŒ…å«äº†ä¸€å¥—å®Œæ•´çš„å¾®è°ƒæµç¨‹ï¼Œç”¨äºè®­ç»ƒè½»é‡åŒ–ä¸”å…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼ˆå¦‚ Qwen2.5-0.5B-Instructï¼‰ã€‚

This project provides not only data generation but also a complete fine-tuning workflow for training lightweight models with reasoning capabilities (e.g., Qwen2.5-0.5B-Instruct).

## ğŸš€ æ¦‚è¿° / Overview

å¾®è°ƒçš„ç›®æ ‡æ˜¯è®©æ¨¡å‹å­¦ä¹ å¦‚ä½•ç†è§£å¤æ‚çš„ä»£ç ä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆé€»è¾‘ä¸¥å¯†çš„æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬é‡‡ç”¨äº†å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ (PEFT)ï¼Œç‰¹åˆ«æ˜¯ LoRAã€‚

The goal of fine-tuning is to enable the model to understand complex code contexts and generate logically sound reasoning traces. We utilize Parameter-Efficient Fine-Tuning (PEFT) techniques, specifically LoRA.

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡ / Environment Setup

å¾®è°ƒè„šæœ¬ä½äº `src/fine-tune/` ç›®å½•ä¸‹ã€‚

The fine-tuning scripts are located in the `src/fine-tune/` directory.

1.  **å®‰è£…ä¾èµ– / Install Dependencies**:
    ```bash
    pip install -r src/fine-tune/requirements.txt
    ```
    æ³¨ï¼šæ¨èåœ¨å…·å¤‡ GPUï¼ˆå¦‚ NVIDIA T4/A100ï¼‰çš„ç¯å¢ƒä¸­è¿è¡Œã€‚
    Note: Running in a GPU environment (e.g., NVIDIA T4/A100) is recommended.

2.  **æ ¸å¿ƒç»„ä»¶ / Core Components**:
    *   `train.py`: åŸºäº Unsloth çš„å¾®è°ƒä¸»è„šæœ¬ã€‚ The main fine-tuning script based on Unsloth.
    *   `app.py`: æä¾› Gradio ç•Œé¢ï¼Œæ”¯æŒè¿œç¨‹è®­ç»ƒã€èŠå¤©ä¸æµ‹è¯•ã€‚ A Gradio interface for remote training, chat, and testing.
    *   `export_gguf.py`: å°†å¾®è°ƒåçš„ LoRA æƒé‡å¯¼å‡ºä¸º GGUF æ ¼å¼ã€‚ Export fine-tuned LoRA weights to GGUF format.

## ğŸ“ˆ è®­ç»ƒé…ç½® / Training Configuration

æˆ‘ä»¬åœ¨è®­ç»ƒä¸­ä½¿ç”¨äº†ä»¥ä¸‹å…³é”®é…ç½®ï¼š
We used the following key configurations during training:

*   **åŸºç¡€æ¨¡å‹ / Base Model**: Qwen2.5-0.5B-Instruct
*   **è®­ç»ƒæ–¹æ³• / Method**: LoRA (Rank 16, Alpha 16)
*   **æ•°æ®é›† / Dataset**: è‡ªåŠ¨ç”Ÿæˆçš„ 1,400+ ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼ˆåŒ…å«æ¨ç†è½¨è¿¹ï¼‰ã€‚
    1,400+ automatically generated high-quality samples (including reasoning traces).
*   **ä¼˜åŒ–å™¨ / Optimizer**: AdamW (8-bit)

## ğŸ§ª æµ‹è¯•ä¸è¯„æµ‹ / Testing & Evaluation

è®­ç»ƒç»“æŸåï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¿è¡Œä¸€å¥—æµ‹è¯•æµç¨‹ã€‚
After training, the system automatically runs a testing workflow.

### æµ‹è¯•æ•°æ® / Test Data
æµ‹è¯•é›†ä½äº `src/fine-tune/data/`ï¼š
*   `test_questions.csv`: åŸºç¡€æµ‹è¯•é—®é¢˜é›†ã€‚
*   `test_questions_fullLength.jsonl`: åŒ…å«å®Œæ•´ä»£ç ä¸Šä¸‹æ–‡çš„è¿›é˜¶æµ‹è¯•é›†ã€‚
*   `test_mapping.md`: é—®é¢˜ä¸æºä»£ç çš„å¯¹åº”å…³ç³»ã€‚

### è¯„æµ‹ç»“æœ / Results
å¾®è°ƒåçš„æ¨¡å‹åœ¨ä»£ç é€»è¾‘è§£é‡Šï¼ˆåœºæ™¯ 1ï¼‰å’Œæ¶æ„è®¾è®¡æ–¹æ¡ˆï¼ˆåœºæ™¯ 2ï¼‰ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„æ¨ç†æ­¥æ•°æå‡ã€‚è¯¦ç»†çš„è¯„æµ‹æŠ¥å‘Šä¿å­˜åœ¨ `data/6.fine_tune_qwen/test_results.txt`ã€‚

The fine-tuned model shows significant improvement in reasoning steps for code logic explanation (Scenario 1) and architectural design (Scenario 2). Detailed evaluation reports are saved in `data/6.fine_tune_qwen/test_results.txt`.

## ğŸ“ äº§ç‰©è¾“å‡º / Output Artifacts

è¾“å‡ºæ–‡ä»¶ä¿å­˜åœ¨ `data/6.fine_tune_qwen/`ï¼š
*   `adapter_model.safetensors`: LoRA æƒé‡æ–‡ä»¶ã€‚
*   `adapter_config.json`: æƒé‡é…ç½®æ–‡ä»¶ã€‚
*   `qwen2.5-0.5b-instruct.Q8_0.gguf`: é‡åŒ–åçš„ GGUF æ¨¡å‹ï¼ˆå¯é€‰ï¼‰ã€‚

---
æ›´å¤šå…³äºå¦‚ä½•è¿è¡Œå¾®è°ƒçš„ä¿¡æ¯ï¼Œè¯·å‚è€ƒ `src/fine-tune/README.md`ã€‚
For more information on how to run the fine-tuning, please refer to `src/fine-tune/README.md`.
