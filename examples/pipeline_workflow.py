"""
Pipeline Workflow Example - Complete End-to-End Process
æµæ°´çº¿å·¥ä½œæµç¤ºä¾‹ - å®Œæ•´çš„ç«¯åˆ°ç«¯è¿‡ç¨‹

This example demonstrates the complete pipeline from code slicing to final dataset compilation.
æ­¤ç¤ºä¾‹æ¼”ç¤ºä»Žä»£ç åˆ‡ç‰‡åˆ°æœ€ç»ˆæ•°æ®é›†ç¼–è¯‘çš„å®Œæ•´æµæ°´çº¿ã€‚
"""

import logging
from pathlib import Path

from src.pipeline.code_slicer import CodeSlicer
from src.pipeline.batch_processor import BatchProcessor
from src.pipeline.dataset_compiler import DatasetCompiler
from src.analyzers.code_analyzer import RepositoryCloner

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    """Main pipeline workflow"""
    
    # Configuration
    # é…ç½®
    REPOS = [
        {
            "url": "https://github.com/nsidnev/fastapi-realworld-example-app.git",
            "name": "nsidnev/fastapi-realworld-example-app"
        },
        # Add more repositories as needed
        # æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šä»“åº“
    ]
    
    CLONE_DIR = "/tmp/datasets"
    MAX_FILES_PER_REPO = 20  # Limit for demonstration
    
    print("=" * 60)
    print("Training Dataset Generation Pipeline")
    print("è®­ç»ƒæ•°æ®é›†ç”Ÿæˆæµæ°´çº¿")
    print("=" * 60)
    
    # ========================================================================
    # STAGE 1: Code Slicing
    # é˜¶æ®µ1ï¼šä»£ç åˆ‡ç‰‡
    # ========================================================================
    print("\n[STAGE 1] Code Slicing / ä»£ç åˆ‡ç‰‡")
    print("-" * 60)
    
    slicer = CodeSlicer(output_dir="slices")
    
    for repo in REPOS:
        logger.info(f"Processing repository: {repo['name']}")
        
        # Clone repository if needed
        # å¦‚æžœéœ€è¦ï¼Œå…‹éš†ä»“åº“
        repo_path = Path(CLONE_DIR) / repo['name'].replace('/', '_')
        if not repo_path.exists():
            logger.info(f"Cloning {repo['url']}...")
            RepositoryCloner.clone(repo['url'], str(repo_path))
        
        # Slice the repository
        # åˆ‡ç‰‡ä»“åº“
        slices = slicer.slice_repository(
            repo_path=str(repo_path),
            repo_name=repo['name'],
            max_files=MAX_FILES_PER_REPO
        )
        logger.info(f"Generated {len(slices)} slices from {repo['name']}")
    
    # Export slices to JSONL
    # å¯¼å‡ºåˆ‡ç‰‡åˆ°JSONL
    slices_file = slicer.export_slices()
    logger.info(f"Slices exported to: {slices_file}")
    
    # Show statistics
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = slicer.get_statistics()
    print("\nSlicing Statistics / åˆ‡ç‰‡ç»Ÿè®¡:")
    print(f"  Total slices: {stats['total_slices']}")
    print(f"  By type: {stats['by_type']}")
    print(f"  By complexity: {stats['by_complexity']}")
    
    print("\nâš ï¸  MANUAL REVIEW CHECKPOINT 1 / äººå·¥å®¡æ ¸æ£€æŸ¥ç‚¹1")
    print("   Please review the slices at:", slices_file)
    print("   Move reviewed slices to 'reviewed_slices/' directory")
    print("   è¯·å®¡æ ¸åˆ‡ç‰‡æ–‡ä»¶:", slices_file)
    print("   å°†å®¡æ ¸åŽçš„åˆ‡ç‰‡ç§»è‡³ 'reviewed_slices/' ç›®å½•")
    
    # ========================================================================
    # STAGE 2: Manual Review (simulated - in practice, user does this)
    # é˜¶æ®µ2ï¼šäººå·¥å®¡æ ¸ï¼ˆæ¨¡æ‹Ÿ - å®žè·µä¸­ç”±ç”¨æˆ·å®Œæˆï¼‰
    # ========================================================================
    print("\n[STAGE 2] Manual Review / äººå·¥å®¡æ ¸")
    print("-" * 60)
    print("In practice, you would:")
    print("1. Review slices in 'slices/' directory")
    print("2. Filter or modify as needed")
    print("3. Save approved slices to 'reviewed_slices/'")
    print("\nå®žè·µä¸­ï¼Œæ‚¨éœ€è¦ï¼š")
    print("1. å®¡æ ¸ 'slices/' ç›®å½•ä¸­çš„åˆ‡ç‰‡")
    print("2. æ ¹æ®éœ€è¦è¿‡æ»¤æˆ–ä¿®æ”¹")
    print("3. å°†æ‰¹å‡†çš„åˆ‡ç‰‡ä¿å­˜åˆ° 'reviewed_slices/'")
    
    # For demo purposes, we'll use the original slices
    # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŽŸå§‹åˆ‡ç‰‡
    reviewed_slices_file = slices_file
    
    # ========================================================================
    # STAGE 3: Batch Processing Preparation
    # é˜¶æ®µ3ï¼šæ‰¹å¤„ç†å‡†å¤‡
    # ========================================================================
    print("\n[STAGE 3] Batch Processing Preparation / æ‰¹å¤„ç†å‡†å¤‡")
    print("-" * 60)
    
    processor = BatchProcessor(
        config_path="llm_config.yaml",
        output_dir="batch_input"
    )
    
    # Load reviewed slices
    # åŠ è½½å®¡æ ¸åŽçš„åˆ‡ç‰‡
    with open(reviewed_slices_file, 'r') as f:
        import json
        reviewed_slices = [json.loads(line) for line in f if line.strip()]
    
    # Split slices for different scenarios
    # ä¸ºä¸åŒåœºæ™¯åˆ†å‰²åˆ‡ç‰‡
    # For Scenario 1: Use function slices (Q&A works better with functions)
    # åœºæ™¯1ï¼šä½¿ç”¨å‡½æ•°åˆ‡ç‰‡ï¼ˆé—®ç­”æ›´é€‚åˆå‡½æ•°ï¼‰
    scenario1_slices = [s for s in reviewed_slices if s['type'] == 'function'][:10]
    
    # For Scenario 2: Use class slices (Design works better with classes)
    # åœºæ™¯2ï¼šä½¿ç”¨ç±»åˆ‡ç‰‡ï¼ˆè®¾è®¡æ›´é€‚åˆç±»ï¼‰
    scenario2_slices = [s for s in reviewed_slices if s['type'] == 'class'][:5]
    
    logger.info(f"Scenario 1 slices: {len(scenario1_slices)}")
    logger.info(f"Scenario 2 slices: {len(scenario2_slices)}")
    
    # Create batch requests for Scenario 1
    # ä¸ºåœºæ™¯1åˆ›å»ºæ‰¹å¤„ç†è¯·æ±‚
    scenario1_requests = processor.create_scenario1_prompts(scenario1_slices)
    scenario1_batch_file = processor.export_batch_requests(
        scenario1_requests, 
        scenario="scenario1"
    )
    logger.info(f"Scenario 1 batch requests exported to: {scenario1_batch_file}")
    
    # Create batch requests for Scenario 2
    # ä¸ºåœºæ™¯2åˆ›å»ºæ‰¹å¤„ç†è¯·æ±‚
    scenario2_requests = processor.create_scenario2_prompts(scenario2_slices)
    scenario2_batch_file = processor.export_batch_requests(
        scenario2_requests,
        scenario="scenario2"
    )
    logger.info(f"Scenario 2 batch requests exported to: {scenario2_batch_file}")
    
    print("\nðŸ“ Next Steps for Batch Processing:")
    print("1. Upload batch files to OpenAI Batch API")
    print("2. Wait for processing (typically 24h)")
    print("3. Download results to 'batch_output/' directory")
    print("\nðŸ“ æ‰¹å¤„ç†çš„åŽç»­æ­¥éª¤ï¼š")
    print("1. å°†æ‰¹å¤„ç†æ–‡ä»¶ä¸Šä¼ åˆ°OpenAIæ‰¹å¤„ç†API")
    print("2. ç­‰å¾…å¤„ç†ï¼ˆé€šå¸¸24å°æ—¶ï¼‰")
    print("3. å°†ç»“æžœä¸‹è½½åˆ° 'batch_output/' ç›®å½•")
    
    # ========================================================================
    # STAGE 4: Manual Review of Generated Data (in practice)
    # é˜¶æ®µ4ï¼šç”Ÿæˆæ•°æ®çš„äººå·¥å®¡æ ¸ï¼ˆå®žè·µä¸­ï¼‰
    # ========================================================================
    print("\n[STAGE 4] Manual Review of Generated Data / ç”Ÿæˆæ•°æ®çš„äººå·¥å®¡æ ¸")
    print("-" * 60)
    print("After batch processing completes:")
    print("1. Review the generated Q&A pairs and design solutions")
    print("2. Filter out low-quality items")
    print("3. Keep approved items for final compilation")
    print("\næ‰¹å¤„ç†å®ŒæˆåŽï¼š")
    print("1. å®¡æ ¸ç”Ÿæˆçš„é—®ç­”å¯¹å’Œè®¾è®¡æ–¹æ¡ˆ")
    print("2. è¿‡æ»¤æŽ‰ä½Žè´¨é‡é¡¹ç›®")
    print("3. ä¿ç•™æ‰¹å‡†çš„é¡¹ç›®ç”¨äºŽæœ€ç»ˆç¼–è¯‘")
    
    # ========================================================================
    # STAGE 5: Dataset Compilation (simulated with dummy data)
    # é˜¶æ®µ5ï¼šæ•°æ®é›†ç¼–è¯‘ï¼ˆä½¿ç”¨è™šæ‹Ÿæ•°æ®æ¨¡æ‹Ÿï¼‰
    # ========================================================================
    print("\n[STAGE 5] Dataset Compilation / æ•°æ®é›†ç¼–è¯‘")
    print("-" * 60)
    print("NOTE: This stage requires actual batch API responses.")
    print("For demonstration, we'll create dummy response files.")
    print("\næ³¨æ„ï¼šæ­¤é˜¶æ®µéœ€è¦å®žé™…çš„æ‰¹å¤„ç†APIå“åº”ã€‚")
    print("ä¸ºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬å°†åˆ›å»ºè™šæ‹Ÿå“åº”æ–‡ä»¶ã€‚")
    
    # Create dummy response files for demonstration
    # ä¸ºæ¼”ç¤ºåˆ›å»ºè™šæ‹Ÿå“åº”æ–‡ä»¶
    _create_dummy_responses()
    
    # Compile the dataset
    # ç¼–è¯‘æ•°æ®é›†
    compiler = DatasetCompiler(output_dir="final_output")
    
    # Load scenario data (if exists)
    # åŠ è½½åœºæ™¯æ•°æ®ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
    scenario1_response = "batch_output/scenario1_responses.jsonl"
    scenario2_response = "batch_output/scenario2_responses.jsonl"
    
    if Path(scenario1_response).exists() and Path(scenario2_response).exists():
        compiler.load_scenario_data(
            scenario1_file=scenario1_response,
            scenario2_file=scenario2_response
        )
        
        # Generate statistics
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats_file = compiler.export_statistics()
        logger.info(f"Statistics exported to: {stats_file}")
        
        # Export training dataset
        # å¯¼å‡ºè®­ç»ƒæ•°æ®é›†
        training_file = compiler.export_training_dataset(shuffle=True, seed=42)
        logger.info(f"Training dataset exported to: {training_file}")
        
        # Create review summary
        # åˆ›å»ºå®¡æ ¸æ‘˜è¦
        summary_file = compiler.create_review_summary()
        logger.info(f"Review summary created at: {summary_file}")
        
        print("\nâœ… Pipeline Complete! / æµæ°´çº¿å®Œæˆï¼")
        print(f"   Training dataset: {training_file}")
        print(f"   Statistics: {stats_file}")
        print(f"   Review summary: {summary_file}")
    else:
        print("\nâš ï¸  Batch response files not found. Skipping compilation.")
        print("   Please process batch requests and place responses in batch_output/")
        print("\nâš ï¸  æœªæ‰¾åˆ°æ‰¹å¤„ç†å“åº”æ–‡ä»¶ã€‚è·³è¿‡ç¼–è¯‘ã€‚")
        print("   è¯·å¤„ç†æ‰¹å¤„ç†è¯·æ±‚å¹¶å°†å“åº”æ”¾åœ¨ batch_output/ ä¸­")


def _create_dummy_responses():
    """Create dummy response files for demonstration"""
    import json
    from pathlib import Path
    
    output_dir = Path("batch_output")
    output_dir.mkdir(exist_ok=True)
    
    # Dummy Scenario 1 response
    scenario1_data = [
        {
            "id": "scenario1_test_00001",
            "scenario": "scenario1",
            "question": "What does this function do?",
            "answer": "This function processes data...",
            "reasoning_trace": {
                "steps": [
                    {
                        "step_number": 1,
                        "description": "Analyze function signature",
                        "code_reference": "def process_data()",
                        "reasoning": "Identifies the function purpose"
                    }
                ],
                "conclusion": "Function performs data processing"
            },
            "business_rules": ["Validates input data"]
        }
    ]
    
    with open(output_dir / "scenario1_responses.jsonl", 'w') as f:
        for item in scenario1_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    # Dummy Scenario 2 response
    scenario2_data = [
        {
            "id": "scenario2_test_00001",
            "scenario": "scenario2",
            "requirement": {
                "title": "User authentication",
                "description": "Implement secure user login",
                "constraints": ["Must be RESTful"]
            },
            "design_solution": {
                "overview": "JWT-based authentication",
                "architecture": {
                    "style": "Layered",
                    "components": ["AuthController", "TokenService"],
                    "data_flow": "Client -> API -> Database"
                }
            },
            "reasoning_trace": {
                "decision_points": [
                    {
                        "decision": "Use JWT tokens",
                        "rationale": "Stateless and scalable"
                    }
                ]
            }
        }
    ]
    
    with open(output_dir / "scenario2_responses.jsonl", 'w') as f:
        for item in scenario2_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')


if __name__ == "__main__":
    main()
